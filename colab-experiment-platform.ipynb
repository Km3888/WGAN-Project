{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZeVBSh7N2e9"
   },
   "source": [
    "This notebook satisfies the environmental requirements of running cGAN experiments in Colab while monitoring via TensorBoard.\n",
    "\n",
    "We start off by making Google Drive files visible to this notebook.\n",
    "We then attach TensorBoard and have it monitor an output directory (LOG_DIR).\n",
    "\n",
    "Finally we incorporate an experimental section from which tests can be run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gN-ODgFbOy3T"
   },
   "source": [
    "# Make Google Drive files accessible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rB0TIg-XQryV"
   },
   "source": [
    "See https://colab.research.google.com/notebooks/io.ipynb#scrollTo=jRQ5_yMcqJiV for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-EaX7RdQ04F"
   },
   "source": [
    "This will prompt you to open a new tab that allows Colab access to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31996,
     "status": "ok",
     "timestamp": 1543370722130,
     "user": {
      "displayName": "Jeff Soules",
      "photoUrl": "",
      "userId": "13521288027814780687"
     },
     "user_tz": 300
    },
    "id": "xfM8677QQk-6",
    "outputId": "4afc9ae5-411b-4764-f86e-859c3e4728cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1488,
     "status": "ok",
     "timestamp": 1543370732155,
     "user": {
      "displayName": "Jeff Soules",
      "photoUrl": "",
      "userId": "13521288027814780687"
     },
     "user_tz": 300
    },
    "id": "PUh6WT2QZ-Sq",
    "outputId": "7c760cf0-a9b9-4fdd-cd2b-ef63ebe31ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'My Drive'  'Team Drives'\n"
     ]
    }
   ],
   "source": [
    "!ls '/content/gdrive/'\n",
    "\n",
    "## UNFORTUNATELY, we cannot mount the filesystem\n",
    "## which ought to be rooted in \"Shared with me\"--i.e. shared files.\n",
    "##\n",
    "## The user will need to copy whatever datasets and imports are needed\n",
    "## and set up directory paths directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3348,
     "status": "ok",
     "timestamp": 1543365327674,
     "user": {
      "displayName": "Kelly Marshall",
      "photoUrl": "",
      "userId": "09151661360907445863"
     },
     "user_tz": 300
    },
    "id": "rM6Gge96RVB2",
    "outputId": "e6138042-7257-416d-85f8-7fe414b66711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Google Drive! I am Colab!"
     ]
    }
   ],
   "source": [
    "# OPTIONAL:\n",
    "# Prove that the mountpoint works. Only needs to be run once.\n",
    "# Also it will overwrite anything in your google drive home directory that\n",
    "# was named foo.txt, so maybe don't run it if that's like a wallet with sixty\n",
    "# bitcoins in it or something\n",
    "with open('/content/gdrive/My Drive/foo.txt', 'w') as f:\n",
    "  f.write('Hello Google Drive! I am Colab!')\n",
    "!cat /content/gdrive/My\\ Drive/foo.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Global Parameters\n",
    "This includes command-line arguments and imports.\n",
    "\n",
    "Note that because a Jupyter notebook, particularly in the cloud environment,\n",
    "does not have a command line per se, flag values are passed as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# SET THESE VALUES TO THE PATHS THAT CORRECTLY DESCRIBE YOUR ENVIRONMENT!\n",
    "DRIVE_BASE_DIR = '/content/gdrive/My Drive/2018-Fa Drori Intro ML/Group Project/'\n",
    "HANDBAG_DIR = DRIVE_BASE_DIR + 'handbags/500_bags'\n",
    "FACADE_DIR = DRIVE_BASE_DIR + 'facades/train'\n",
    "\n",
    "CROP_SIZE = 128\n",
    "\n",
    "if not os.path.isdir(HANDBAG_DIR): raise Exception(\"Default data path not set.\")\n",
    "if not os.path.isdir(FACADE_DIR): raise Exception(\"Default data path not set.\")\n",
    "\n",
    "## NOTE: When running in a notebook or on Colab, there's no easy way to\n",
    "## specify command-line arguments. So we use an ARGSTRING instead.\n",
    "ARGSTRING = [\"--seed=1234\"]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_dir\", help=\"path to folder containing images\")\n",
    "parser.add_argument(\"--output_dir\", help=\"where to put output files\")\n",
    "parser.add_argument(\"--seed\", type=int)\n",
    "parser.add_argument(\"--dataset\", choices=['handbags', 'facades'], default='handbags',\n",
    "                    help=\"Choose dataset to use.\")\n",
    "\n",
    "parser.add_argument(\"--loss_fn\", choices=['wgan', 'mod', 'minimax', 'pix2pix'],\n",
    "                    default='pix2pix',\n",
    "                    help=\"Loss function to use for generator and discriminator.\")\n",
    "\n",
    "parser.add_argument(\"--max_steps\", type=int, default=\"5000\",\n",
    "                    help=\"number of training steps (0 to disable)\")\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=\"20\",\n",
    "                    help=\"number of training epochs\")\n",
    "parser.add_argument(\"--summary_freq\", type=int, default=100, help=\"update summaries every summary_freq steps\")\n",
    "parser.add_argument(\"--progress_freq\", type=int, default=50, help=\"display progress every progress_freq steps\")\n",
    "parser.add_argument(\"--trace_freq\", type=int, default=0, help=\"trace execution every trace_freq steps\")\n",
    "parser.add_argument(\"--display_freq\", type=int, default=10, help=\"write current training images every display_freq steps\")\n",
    "parser.add_argument(\"--save_freq\", type=int, default=5000, help=\"save model every save_freq steps, 0 to disable\")\n",
    "\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1, help=\"number of images in batch\")\n",
    "parser.add_argument(\"--ngf\", type=int, default=64, help=\"number of generator filters in first conv layer\")\n",
    "parser.add_argument(\"--ndf\", type=int, default=64, help=\"number of discriminator filters in first conv layer\")\n",
    "parser.add_argument(\"--scale_size\", type=int, default=140, help=\"scale images to this size before cropping to CROPSIZE^2\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"initial learning rate for adam\")\n",
    "parser.add_argument(\"--beta1\", type=float, default=0.5, help=\"momentum term of adam\")\n",
    "parser.add_argument(\"--l1_weight\", type=float, default=100.0, help=\"weight on L1 term for generator gradient\")\n",
    "parser.add_argument(\"--gan_weight\", type=float, default=1.0, help=\"weight on GAN term for generator gradient\")\n",
    "\n",
    "if len(ARGSTRING) > 0:\n",
    "    a = parser.parse_args(ARGSTRING)\n",
    "else:\n",
    "    a = parser.parse_args()\n",
    "\n",
    "if a.output_dir == \"\" or a.output_dir is None:\n",
    "    a.output_dir = (\"{}-s{}-e{}-batch{}-lr{}-L1wt{}-seed{}-{}\"\n",
    "                    .format(a.dataset, a.max_steps, a.max_epochs, a.batch_size,\n",
    "                            a.lr, a.l1_weight, a.seed, a.loss_fn))\n",
    "    i = 1\n",
    "    stub = a.output_dir\n",
    "    while(os.path.exists(a.output_dir)):\n",
    "        a.output_dir = \"{}.{}\".format(stub, str(i).zfill(4))\n",
    "        i = i + 1\n",
    "    \n",
    "if a.dataset == 'handbags':\n",
    "    a.input_dir = HANDBAG_DIR\n",
    "elif a.dataset == 'facades':\n",
    "    a.input_dir = FACADE_DIR\n",
    "else:\n",
    "    raise ValueError(\"Unknown dataset.\")\n",
    "\n",
    "print(\"Input: {} Output: {}\".format(a.input_dir, a.output_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PySjYFhMO3PF"
   },
   "source": [
    "# Connect TensorBoard to Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86N4ar4s5qjS"
   },
   "outputs": [],
   "source": [
    "### Here's how you'd do it with ngrok, just here for reference.\n",
    "### We used localtunnel.\n",
    "#!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "#!unzip ngrok-stable-linux-amd64.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tvinklq57aL"
   },
   "outputs": [],
   "source": [
    "#import time\n",
    "#get_ipython().system_raw('./ngrok http 6006 &')\n",
    "#time.sleep(2)\n",
    "\n",
    "#! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "#    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21527,
     "status": "ok",
     "timestamp": 1543365363700,
     "user": {
      "displayName": "Kelly Marshall",
      "photoUrl": "",
      "userId": "09151661360907445863"
     },
     "user_tz": 300
    },
    "id": "NkDVmF-l5XZ-",
    "outputId": "c88fc40f-0b3b-4c36-ef1c-b3b6cd66f04e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25h/tools/node/bin/npm -> /tools/node/lib/node_modules/npm/bin/npm-cli.js\n",
      "\u001b[K\u001b[?25h/tools/node/bin/npx -> /tools/node/lib/node_modules/npm/bin/npx-cli.js\n",
      "\u001b[K\u001b[?25h+ npm@6.4.1\n",
      "added 273 packages from 152 contributors, removed 419 packages and updated 40 packages in 13.862s\n",
      "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/client\n",
      "\u001b[K\u001b[?25h+ localtunnel@1.9.1\n",
      "added 54 packages from 31 contributors in 2.45s\n"
     ]
    }
   ],
   "source": [
    "## Install requirements\n",
    "# (Only need to execute this cell once)\n",
    "! npm i -g npm\n",
    "! npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8533,
     "status": "ok",
     "timestamp": 1543365384517,
     "user": {
      "displayName": "Kelly Marshall",
      "photoUrl": "",
      "userId": "09151661360907445863"
     },
     "user_tz": 300
    },
    "id": "KONzMIB853x4",
    "outputId": "fc939a79-0794-41a9-d68c-ed04725aa743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: tensorboard --logdir /content/gdrive/My Drive/Group Project/log-fullRun/ --host 0.0.0.0 --port 6006 &\n",
      "Connect to TensorBoard from wherever at this URL:\n",
      "your url is: https://hot-grasshopper-20.localtunnel.me\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Now launch TensorBoard pointing to the globally configured log directory.\n",
    "print(\"Executing: tensorboard --logdir {} --host 0.0.0.0 --port 6006 &\"\n",
    "  .format(a.output_dir))\n",
    "# Shell out to execute tensorboard\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir \"{}\" --host 0.0.0.0 --port 6006 &'\n",
    "    .format(a.output_dir)\n",
    ")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Tunnel port 6006 (assume TensorBoard ran successfully)\n",
    "get_ipython().system_raw('lt --port 6006 > url.txt 2>&1 &')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "print (\"Connect to TensorBoard from wherever at this URL:\")\n",
    "! cat url.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qP3RTpREWFHG"
   },
   "source": [
    "If you change the logging directory, you'll need to kill\n",
    "the existing TensorBoard process and start\n",
    "a new one. (So far as I can tell, restarting won't succeed; there can only be one\n",
    "process listening on a port at a time anyway.)\n",
    "\n",
    "The following cell will do that (you need to run it, look at the output, then modify the \"kill\" line and uncomment it so it will actually kill the process.) The rerun of ps is a double-check that you did successfully shut TensorBoard down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9182,
     "status": "ok",
     "timestamp": 1543273705731,
     "user": {
      "displayName": "Kelly Marshall",
      "photoUrl": "",
      "userId": "09151661360907445863"
     },
     "user_tz": 300
    },
    "id": "NLiPtrZ0WXYn",
    "outputId": "34d6ac69-e7c3-4529-af36-4b2802620686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PID TTY          TIME CMD\n",
      "    291 ?        00:02:54 tensorboard\n",
      "    291 ?        00:02:55 tensorboard\n"
     ]
    }
   ],
   "source": [
    "## RUN THIS CELL ONLY IF YOU NEED TO RESTART TENSORBOARD (like to point to a new log dir)\n",
    "import time\n",
    "!ps | head -n 1\n",
    "!ps | grep 'tensorboard'\n",
    "## Read the \"PID\" column to figure out what process number TensorBoard is,\n",
    "## then use that processs in the next line\n",
    "# !kill PID_GOES_HERE\n",
    "time.sleep(2)\n",
    "!ps | grep 'tensorboard'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "-MQSA-gS-JEX"
   },
   "source": [
    "The following cell cleans out the log directory *LOG_DIR*.\n",
    "\n",
    "Uncomment the \"os.unlink\" and \"shutil.rmtree\" lines to actually delete\n",
    "(they're commented as safety so you don't delete work by running the cell accidentally.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1543273708070,
     "user": {
      "displayName": "Kelly Marshall",
      "photoUrl": "",
      "userId": "09151661360907445863"
     },
     "user_tz": 300
    },
    "id": "qU9t94gP7ak5",
    "outputId": "2f0b5bee-f887-450e-9d27-411322f6cb42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlinking /content/gdrive/My Drive/Group Project/log-fullRun/graph.pbtxt\n",
      "Recursively removing /content/gdrive/My Drive/Group Project/log-fullRun/images\n",
      "Unlinking /content/gdrive/My Drive/Group Project/log-fullRun/index.html\n",
      "Unlinking /content/gdrive/My Drive/Group Project/log-fullRun/events.out.tfevents.1543241235.59feb57c6f7e\n",
      "Unlinking /content/gdrive/My Drive/Group Project/log-fullRun/events.out.tfevents.1543271338.943eaeaf319f\n",
      "Unlinking /content/gdrive/My Drive/Group Project/log-fullRun/events.out.tfevents.1543271566.943eaeaf319f\n",
      "Unlinking /content/gdrive/My Drive/Group Project/log-fullRun/events.out.tfevents.1543273566.943eaeaf319f\n"
     ]
    }
   ],
   "source": [
    "# Run this ONLY to clear out old log files.\n",
    "# Commented out the actual deleters to avoid accidental execution.\n",
    "import os, shutil\n",
    "for the_file in os.listdir(LOG_DIR):\n",
    "    file_path = os.path.join(LOG_DIR, the_file)\n",
    "    try:\n",
    "        if os.path.isfile(file_path):\n",
    "            print(\"Unlinking {}\".format(file_path))\n",
    "#            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            print (\"Recursively removing {}\".format(file_path))\n",
    "#            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZyZIFGW-uIE"
   },
   "source": [
    "## Test It\n",
    "\n",
    "Only need to execute this to prove that the tensorboard connection is working.\n",
    "\n",
    "If you do run this, be sure to clean out LOG_DIR afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1006
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36169,
     "status": "ok",
     "timestamp": 1543270571269,
     "user": {
      "displayName": "Kelly Marshall",
      "photoUrl": "",
      "userId": "09151661360907445863"
     },
     "user_tz": 300
    },
    "id": "qyclV58D51l7",
    "outputId": "f6e92ee3-1d02-4b1c-ccd2-6acf29082467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-cf6a558d68e8>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: 0001 cost= 1.184038759\n",
      "Epoch: 0002 cost= 0.665434184\n",
      "Epoch: 0003 cost= 0.552909983\n",
      "Epoch: 0004 cost= 0.498737923\n",
      "Epoch: 0005 cost= 0.465547395\n",
      "Epoch: 0006 cost= 0.442658992\n",
      "Epoch: 0007 cost= 0.425608423\n",
      "Epoch: 0008 cost= 0.412207377\n",
      "Epoch: 0009 cost= 0.401463278\n",
      "Epoch: 0010 cost= 0.392418534\n",
      "Epoch: 0011 cost= 0.384726579\n",
      "Epoch: 0012 cost= 0.378187135\n",
      "Epoch: 0013 cost= 0.372420243\n",
      "Epoch: 0014 cost= 0.367342757\n",
      "Epoch: 0015 cost= 0.362765909\n",
      "Epoch: 0016 cost= 0.358620076\n",
      "Epoch: 0017 cost= 0.354928667\n",
      "Epoch: 0018 cost= 0.351470063\n",
      "Epoch: 0019 cost= 0.348320921\n",
      "Epoch: 0020 cost= 0.345376467\n",
      "Epoch: 0021 cost= 0.342740766\n",
      "Epoch: 0022 cost= 0.340266723\n",
      "Epoch: 0023 cost= 0.337926292\n",
      "Epoch: 0024 cost= 0.335771112\n",
      "Epoch: 0025 cost= 0.333646116\n",
      "Training done!\n",
      "Accuracy: 0.9134\n",
      "Now try connecting to the address from the previous cell.\n"
     ]
    }
   ],
   "source": [
    "#### EXAMPLE JUST TO PROVE IT'S WORKING\n",
    "### DON'T RUN THIS UNLESS YOU JUST WANT TO TEST TENSORBOARD-IN-THE-CLOUD\n",
    "### IF YOU DO, RUN THE \"CLEAN OUT LOG DIR\" CELL AFTERWARD\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./tmp/data/\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_epoch = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "with tf.name_scope('Model'):\n",
    "    pred = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "with tf.name_scope('Loss'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "tf.summary.scalar(\"accuracy\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(LOG_DIR, graph=tf.get_default_graph())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # optimize/backprop, do cost (compute loss), summarize\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            avg_cost += c / total_batch\n",
    "        if (epoch+1) % display_epoch == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\",\n",
    "                  \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print (\"Training done!\")\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y:mnist.test.labels}))\n",
    "    print(\"Now try connecting to the address from the previous cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgI7H5NS-9ma"
   },
   "source": [
    "# Experiment Code\n",
    "\n",
    "This is the main code for whatever you want to test!\n",
    "If you have used a custom argument parser above, you could use\n",
    "whatever experiment you want to run down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples = collections.namedtuple(\"Examples\", \"paths, inputs, targets, count, steps_per_epoch\")\n",
    "Model = collections.namedtuple(\"Model\", \"outputs, predict_real, predict_fake, discrim_loss, discrim_grads_and_vars, gen_loss_GAN, gen_loss_L1, gen_grads_and_vars, train\")\n",
    "\n",
    "\n",
    "def preprocess(image):\n",
    "    with tf.name_scope(\"preprocess\"):\n",
    "        # [0, 1] => [-1, 1]\n",
    "        return image * 2 - 1\n",
    "\n",
    "\n",
    "def discrim_conv(batch_input, out_channels, stride):\n",
    "    padded_input = tf.pad(batch_input, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"CONSTANT\")\n",
    "    return tf.layers.conv2d(padded_input, out_channels, kernel_size=4, strides=(stride, stride), padding=\"valid\", kernel_initializer=tf.random_normal_initializer(0, 0.02))\n",
    "\n",
    "\n",
    "def gen_conv(batch_input, out_channels):\n",
    "    # [batch, in_height, in_width, in_channels] => [batch, out_height, out_width, out_channels]\n",
    "    initializer = tf.random_normal_initializer(0, 0.02)\n",
    "    return tf.layers.conv2d(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=\"same\", kernel_initializer=initializer)\n",
    "\n",
    "\n",
    "def gen_deconv(batch_input, out_channels):\n",
    "    # [batch, in_height, in_width, in_channels] => [batch, out_height, out_width, out_channels]\n",
    "    initializer = tf.random_normal_initializer(0, 0.02)\n",
    "    return tf.layers.conv2d_transpose(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=\"same\", kernel_initializer=initializer)\n",
    "\n",
    "\n",
    "def lrelu(x, a):\n",
    "    with tf.name_scope(\"lrelu\"):\n",
    "        # adding these together creates the leak part and linear part\n",
    "        # then cancels them out by subtracting/adding an absolute value term\n",
    "        # leak: a*x/2 - a*abs(x)/2\n",
    "        # linear: x/2 + abs(x)/2\n",
    "\n",
    "        # this block looks like it has 2 inputs on the graph unless we do this\n",
    "        x = tf.identity(x)\n",
    "        return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)\n",
    "\n",
    "\n",
    "def batchnorm(inputs):\n",
    "    return tf.layers.batch_normalization(inputs, axis=3, epsilon=1e-5, momentum=0.1, training=True, gamma_initializer=tf.random_normal_initializer(1.0, 0.02))\n",
    "\n",
    "\n",
    "def load_examples():\n",
    "    if a.input_dir is None or not os.path.exists(a.input_dir):\n",
    "        raise Exception(\"input_dir does not exist\")\n",
    "\n",
    "    input_paths = glob.glob(os.path.join(a.input_dir, \"*.jpg\"))\n",
    "    decode = tf.image.decode_jpeg\n",
    "\n",
    "    if len(input_paths) == 0:\n",
    "        raise Exception(\"input_dir contains no image files\")\n",
    "\n",
    "    def get_name(path):\n",
    "        name, _ = os.path.splitext(os.path.basename(path))\n",
    "        return name\n",
    "\n",
    "    # if the image names are numbers, sort by the value rather than asciibetically\n",
    "    # having sorted inputs means that the outputs are sorted in test mode\n",
    "    if all(get_name(path).isdigit() for path in input_paths):\n",
    "        input_paths = sorted(input_paths, key=lambda path: int(get_name(path)))\n",
    "    else:\n",
    "        input_paths = sorted(input_paths)\n",
    "\n",
    "    with tf.name_scope(\"load_images\"):\n",
    "        path_queue = tf.train.string_input_producer(input_paths, shuffle=True)\n",
    "        reader = tf.WholeFileReader()\n",
    "        paths, contents = reader.read(path_queue)\n",
    "        raw_input = decode(contents)\n",
    "        raw_input = tf.image.convert_image_dtype(raw_input, dtype=tf.float32)\n",
    "\n",
    "        assertion = tf.assert_equal(tf.shape(raw_input)[2], 3, message=\"image does not have 3 channels\")\n",
    "        with tf.control_dependencies([assertion]):\n",
    "            raw_input = tf.identity(raw_input)\n",
    "\n",
    "        raw_input.set_shape([None, None, 3])\n",
    "\n",
    "        # break apart image pair and move to range [-1, 1]\n",
    "        width = tf.shape(raw_input)[1] # [height, width, channels]\n",
    "        left_images = preprocess(raw_input[:,:width//2,:])\n",
    "        right_images = preprocess(raw_input[:,width//2:,:])\n",
    "\n",
    "    if a.dataset == 'handbags':\n",
    "        inputs, targets = [left_images, right_images]\n",
    "    elif a.dataset == 'facades':\n",
    "        inputs, targets = [right_images, left_images]\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized data set requested.\")\n",
    "\n",
    "    # synchronize seed for image operations so that we do the same operations to both\n",
    "    # input and output images\n",
    "    seed = random.randint(0, 2**31 - 1)\n",
    "    def transform(image):\n",
    "        r = image\n",
    "\n",
    "        # area produces a nice downscaling, but does nearest neighbor for upscaling\n",
    "        # assume we're going to be doing downscaling here\n",
    "        r = tf.image.resize_images(r, [a.scale_size, a.scale_size], method=tf.image.ResizeMethod.AREA)\n",
    "\n",
    "        offset = tf.cast(tf.floor(tf.random_uniform([2], 0, a.scale_size - CROP_SIZE + 1, seed=seed)), dtype=tf.int32)\n",
    "        if a.scale_size > CROP_SIZE:\n",
    "            r = tf.image.crop_to_bounding_box(r, offset[0], offset[1], CROP_SIZE, CROP_SIZE)\n",
    "        elif a.scale_size < CROP_SIZE:\n",
    "            raise Exception(\"scale size cannot be less than crop size\")\n",
    "        return r\n",
    "\n",
    "    with tf.name_scope(\"input_images\"):\n",
    "        input_images = transform(inputs)\n",
    "\n",
    "    with tf.name_scope(\"target_images\"):\n",
    "        target_images = transform(targets)\n",
    "\n",
    "    paths_batch, inputs_batch, targets_batch = tf.train.batch([paths, input_images, target_images], batch_size=a.batch_size)\n",
    "    steps_per_epoch = int(math.ceil(len(input_paths) / a.batch_size))\n",
    "\n",
    "    return Examples(\n",
    "        paths=paths_batch,\n",
    "        inputs=inputs_batch,\n",
    "        targets=targets_batch,\n",
    "        count=len(input_paths),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_generator(generator_inputs, generator_outputs_channels):\n",
    "    layers = []\n",
    "\n",
    "    # encoder_1: [batch, 256, 256, in_channels] => [batch, 128, 128, ngf]\n",
    "    # or [batch, 128, 128, in_channels] => [batch, 64, 64, ngf], or whatever!\n",
    "    with tf.variable_scope(\"encoder_1\"):\n",
    "        output = gen_conv(generator_inputs, a.ngf)\n",
    "        layers.append(output)\n",
    "\n",
    "    # The following automatically accommodates images of arbitrary size\n",
    "    # provided the image dimension is a power-of-2.\n",
    "    encoder_layer_specs = []\n",
    "    for i in range (1, int(math.log(CROP_SIZE, 2))):\n",
    "        v = a.ngf * (min(2**i, 8))\n",
    "        encoder_layer_specs.append(v)\n",
    "        \n",
    "#     layer_specs = [\n",
    "#         a.ngf * 2, # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]\n",
    "#         a.ngf * 4, # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]\n",
    "#         a.ngf * 8, # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]\n",
    "#         a.ngf * 8, # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]\n",
    "#         a.ngf * 8, # encoder_6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]\n",
    "#         a.ngf * 8, # encoder_7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]\n",
    "\n",
    "#         ### We shrunk the image size, so have to drop a layer!!\n",
    "#         ### Above sizes are off by a factor of 2!\n",
    "# #        a.ngf * 8, # encoder_8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]\n",
    "#     ]\n",
    "#     assert(layer_specs == encoder_layer_specs)\n",
    "\n",
    "    for out_channels in encoder_layer_specs:\n",
    "        with tf.variable_scope(\"encoder_%d\" % (len(layers) + 1)):\n",
    "            rectified = lrelu(layers[-1], 0.2)\n",
    "            # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]\n",
    "            convolved = gen_conv(rectified, out_channels)\n",
    "            output = batchnorm(convolved)\n",
    "            layers.append(output)\n",
    "\n",
    "    decoder_layer_specs = []\n",
    "    for i in range (len(encoder_layer_specs), 0, -1):\n",
    "        x = 0.0\n",
    "        if i > 4: x = 0.5\n",
    "        v = (a.ngf * (min(2**(i-1), 8)), x)\n",
    "        decoder_layer_specs.append(v)\n",
    "            \n",
    "#     layer_specs = [\n",
    "#         ### and the reverse of the above: drop a layer coming back out\n",
    "# #        (a.ngf * 8, 0.5),   # decoder_8: [batch, 1, 1, ngf * 8] => [batch, 2, 2, ngf * 8 * 2]\n",
    "#         (a.ngf * 8, 0.5),   # decoder_7: [batch, 2, 2, ngf * 8 * 2] => [batch, 4, 4, ngf * 8 * 2]\n",
    "#         (a.ngf * 8, 0.5),   # decoder_6: [batch, 4, 4, ngf * 8 * 2] => [batch, 8, 8, ngf * 8 * 2]\n",
    "#         (a.ngf * 8, 0.0),   # decoder_5: [batch, 8, 8, ngf * 8 * 2] => [batch, 16, 16, ngf * 8 * 2]\n",
    "#         (a.ngf * 4, 0.0),   # decoder_4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]\n",
    "#         (a.ngf * 2, 0.0),   # decoder_3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]\n",
    "#         (a.ngf, 0.0),       # decoder_2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]\n",
    "#     ]\n",
    "#     assert(layer_specs == decoder_layer_specs)\n",
    "    \n",
    "    num_encoder_layers = len(layers)\n",
    "    for decoder_layer, (out_channels, dropout) in enumerate(decoder_layer_specs):\n",
    "        skip_layer = num_encoder_layers - decoder_layer - 1\n",
    "        with tf.variable_scope(\"decoder_%d\" % (skip_layer + 1)):\n",
    "            if decoder_layer == 0:\n",
    "                # first decoder layer doesn't have skip connections\n",
    "                # since it is directly connected to the skip_layer\n",
    "                input = layers[-1]\n",
    "            else:\n",
    "                for i in range(0, len(layers)):\n",
    "                    print(\"\\t{}:\\n{}\\n\"\n",
    "                          .format(i, layers[i]))\n",
    "                input = tf.concat([layers[-1], layers[skip_layer]], axis=3)\n",
    "\n",
    "            rectified = tf.nn.relu(input)\n",
    "            # [batch, in_height, in_width, in_channels] => [batch, in_height*2, in_width*2, out_channels]\n",
    "            output = gen_deconv(rectified, out_channels)\n",
    "            output = batchnorm(output)\n",
    "\n",
    "            if dropout > 0.0:\n",
    "                output = tf.nn.dropout(output, keep_prob=1 - dropout)\n",
    "\n",
    "            layers.append(output)\n",
    "\n",
    "    # decoder_1: [batch, CROP_SIZE/2, CROP_SIZE/2, ngf * 2] =>\n",
    "    #            [batch, CROP_SIZE, CROP_SIZE, generator_outputs_channels]\n",
    "    with tf.variable_scope(\"decoder_1\"):\n",
    "        input = tf.concat([layers[-1], layers[0]], axis=3)\n",
    "        rectified = tf.nn.relu(input)\n",
    "        output = gen_deconv(rectified, generator_outputs_channels)\n",
    "        output = tf.tanh(output)\n",
    "        layers.append(output)\n",
    "\n",
    "    return layers[-1]\n",
    "\n",
    "\n",
    "\n",
    "######################### LOSS FUNCTIONS #######################################\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "## pix2pix losses are a functionalization of the original Hesse code.\n",
    "def pix2pix_disc_loss(predict_real, predict_fake):\n",
    "    with tf.name_scope(\"pix2pix_discriminator_loss\"):\n",
    "        # minimizing -tf.log will try to get inputs to 1\n",
    "        # predict_real => 1\n",
    "        # predict_fake => 0\n",
    "        discrim_loss = tf.reduce_mean(-(tf.log(predict_real + EPS)\n",
    "                                        + tf.log(1 - predict_fake + EPS)))\n",
    "    return discrim_loss\n",
    "\n",
    "\n",
    "def pix2pix_gen_GAN_loss(predict_fake):\n",
    "    with tf.name_scope(\"pix2pix_generator_loss\"):\n",
    "        # predict_fake => 1\n",
    "        # abs(targets - outputs) => 0\n",
    "        gen_loss_GAN = tf.reduce_mean(-tf.log(predict_fake + EPS))\n",
    "    return gen_loss_GAN\n",
    "\n",
    "\n",
    "## The following loss functions (wasserstein_discriminator_loss, wasserstein_generator_loss,\n",
    "# minimax_discriminator_loss, minimax_generator_loss, modified_generator_loss)\n",
    "# adapted from Shor. Original code at:\n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/losses/python/losses_impl.py\n",
    "# That code originally Copyright 2017 The TensorFlow Authors and licensed under the Apache License.\n",
    "# See http://www.apache.org/licenses/LICENSE-2.0.\n",
    "\n",
    "def wasserstein_discriminator_loss(\n",
    "        discriminator_real_outputs,\n",
    "        discriminator_gen_outputs):\n",
    "    # For this experiment we weight real/generated losses equally\n",
    "    real_weights = 1.0\n",
    "    generated_weights = 1.0\n",
    "    loss_collection = tf.GraphKeys.LOSSES\n",
    "    reduction = tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS\n",
    "    \n",
    "    with tf.name_scope(None, 'discriminator_wasserstein_loss', (\n",
    "            discriminator_real_outputs, discriminator_gen_outputs, real_weights,\n",
    "            generated_weights)) as scope:\n",
    "        discriminator_real_outputs = tf.to_float(discriminator_real_outputs)\n",
    "        discriminator_gen_outputs = tf.to_float(discriminator_gen_outputs)\n",
    "        discriminator_real_outputs.shape.assert_is_compatible_with(\n",
    "            discriminator_gen_outputs.shape)\n",
    "        \n",
    "        loss_on_generated = tf.losses.compute_weighted_loss(\n",
    "            discriminator_gen_outputs, generated_weights, scope,\n",
    "            loss_collection=None, reduction=reduction)\n",
    "        loss_on_real = tf.losses.compute_weighted_loss(\n",
    "            discriminator_real_outputs, real_weights, scope, loss_collection=None,\n",
    "            reduction=reduction)\n",
    "        loss = loss_on_generated - loss_on_real\n",
    "        tf.losses.add_loss(loss, loss_collection)\n",
    "        \n",
    "        tf.summary.scalar('discriminator_gen_wass_loss', loss_on_generated)\n",
    "        tf.summary.scalar('discriminator_real_wass_loss', loss_on_real)\n",
    "        tf.summary.scalar('discriminator_wass_loss', loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def wasserstein_generator_loss(discriminator_gen_outputs):\n",
    "    weights = 1.0\n",
    "    loss_collection = tf.GraphKeys.LOSSES\n",
    "    reduction = tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS\n",
    "  \n",
    "    with tf.name_scope(None, 'generator_wasserstein_loss', (\n",
    "            discriminator_gen_outputs, weights)) as scope:\n",
    "        discriminator_gen_outputs = tf.to_float(discriminator_gen_outputs)\n",
    "\n",
    "        loss = - discriminator_gen_outputs\n",
    "        loss = tf.losses.compute_weighted_loss(\n",
    "            loss, weights, scope, loss_collection, reduction)\n",
    "\n",
    "        tf.summary.scalar('generator_wass_loss', loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def minimax_discriminator_loss(discriminator_real_outputs,\n",
    "                               discriminator_gen_outputs,\n",
    "                               modified = False):\n",
    "    label_smoothing = 0.25\n",
    "    real_weights = 1.0\n",
    "    generated_weights = 1.0\n",
    "    loss_collection = tf.GraphKeys.LOSSES\n",
    "    reduction = tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS\n",
    "    scope_name = 'discriminator_minimax_loss'\n",
    "    if modified:\n",
    "        scope_name = 'discriminator_modified_loss'\n",
    "    with tf.name_scope(None, scope_name, (\n",
    "            discriminator_real_outputs, discriminator_gen_outputs, real_weights,\n",
    "            generated_weights, label_smoothing)) as scope:\n",
    "\n",
    "        # -log((1 - label_smoothing) - sigmoid(D(x)))\n",
    "        loss_on_real = tf.losses.sigmoid_cross_entropy(\n",
    "            tf.ones_like(discriminator_real_outputs),\n",
    "            discriminator_real_outputs, real_weights, label_smoothing, scope,\n",
    "            loss_collection=None, reduction=reduction)\n",
    "        # -log(- sigmoid(D(G(x))))\n",
    "        loss_on_generated = tf.losses.sigmoid_cross_entropy(\n",
    "            tf.zeros_like(discriminator_gen_outputs),\n",
    "            discriminator_gen_outputs, generated_weights, scope=scope,\n",
    "            loss_collection=None, reduction=reduction)\n",
    "\n",
    "        loss = loss_on_real + loss_on_generated\n",
    "        tf.losses.add_loss(loss, loss_collection)\n",
    "\n",
    "        tf.summary.scalar('discriminator_gen_minimax_loss', loss_on_generated)\n",
    "        tf.summary.scalar('discriminator_real_minimax_loss', loss_on_real)\n",
    "        tf.summary.scalar('discriminator_minimax_loss', loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def minimax_generator_loss(discriminator_gen_outputs):\n",
    "    with tf.name_scope(None, 'generator_minimax_loss') as scope:\n",
    "        loss = - minimax_discriminator_loss(\n",
    "            tf.ones_like(discriminator_gen_outputs),\n",
    "            discriminator_gen_outputs)\n",
    "        tf.summary.scalar('generator_minimax_loss', loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def modified_discriminator_loss(discriminator_real_outputs,\n",
    "                                discriminator_gen_outputs):\n",
    "    return minimax_discriminator_loss(discriminator_real_outputs,\n",
    "                                      discriminator_gen_outputs,\n",
    "                                      modified = True)\n",
    "\n",
    "\n",
    "def modified_generator_loss(discriminator_gen_outputs):\n",
    "    label_smoothing = 0.0\n",
    "    weights = 1.0\n",
    "    loss_collection = tf.GraphKeys.LOSSES\n",
    "    reduction = tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS\n",
    "\n",
    "    with tf.name_scope(None, 'generator_modified_loss',\n",
    "                        [discriminator_gen_outputs]) as scope:\n",
    "        loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(discriminator_gen_outputs),\n",
    "                                               discriminator_gen_outputs,\n",
    "                                               weights,\n",
    "                                               label_smoothing,\n",
    "                                               scope,\n",
    "                                               loss_collection,\n",
    "                                               reduction)\n",
    "        tf.summary.scalar('generator_modified_loss', loss)\n",
    "    return loss\n",
    "\n",
    "  ## Assign appropriate loss function to variable\n",
    "DISCRIM_LOSS = None\n",
    "GEN_LOSS = None\n",
    "if (a.loss_fn == 'wgan'):\n",
    "    DISCRIM_LOSS = wasserstein_discriminator_loss\n",
    "    GEN_LOSS = wasserstein_generator_loss\n",
    "elif (a.loss_fn == 'mod'):\n",
    "    DISCRIM_LOSS = modified_discriminator_loss\n",
    "    GEN_LOSS = modified_generator_loss\n",
    "elif (a.loss_fn == 'minimax'):\n",
    "    DISCRIM_LOSS = minimax_discriminator_loss\n",
    "    GEN_LOSS = minimax_generator_loss\n",
    "elif (a.loss_fn == 'pix2pix'):\n",
    "    DISCRIM_LOSS = pix2pix_disc_loss\n",
    "    GEN_LOSS = pix2pix_gen_GAN_loss\n",
    "else:\n",
    "    raise ValueError(\"Unrecognized loss function requested.\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "\n",
    "def create_model(inputs, targets):\n",
    "    def create_discriminator(discrim_inputs, discrim_targets):\n",
    "        n_layers = 3\n",
    "        layers = []\n",
    "\n",
    "        # 2x [batch, height, width, in_channels] => [batch, height, width, in_channels * 2]\n",
    "        input = tf.concat([discrim_inputs, discrim_targets], axis=3)\n",
    "\n",
    "        # layer_1: [batch, 256, 256, in_channels * 2] => [batch, 128, 128, ndf]\n",
    "        with tf.variable_scope(\"layer_1\"):\n",
    "            convolved = discrim_conv(input, a.ndf, stride=2)\n",
    "            rectified = lrelu(convolved, 0.2)\n",
    "            layers.append(rectified)\n",
    "\n",
    "        # layer_2: [batch, 128, 128, ndf] => [batch, 64, 64, ndf * 2]\n",
    "        # layer_3: [batch, 64, 64, ndf * 2] => [batch, 32, 32, ndf * 4]\n",
    "        # layer_4: [batch, 32, 32, ndf * 4] => [batch, 31, 31, ndf * 8]\n",
    "        for i in range(n_layers):\n",
    "            with tf.variable_scope(\"layer_%d\" % (len(layers) + 1)):\n",
    "                out_channels = a.ndf * min(2**(i+1), 8)\n",
    "                stride = 1 if i == n_layers - 1 else 2  # last layer here has stride 1\n",
    "                convolved = discrim_conv(layers[-1], out_channels, stride=stride)\n",
    "                normalized = batchnorm(convolved)\n",
    "                rectified = lrelu(normalized, 0.2)\n",
    "                layers.append(rectified)\n",
    "\n",
    "        # layer_5: [batch, 31, 31, ndf * 8] => [batch, 30, 30, 1]\n",
    "        with tf.variable_scope(\"layer_%d\" % (len(layers) + 1)):\n",
    "            convolved = discrim_conv(rectified, out_channels=1, stride=1)\n",
    "            output = tf.sigmoid(convolved)\n",
    "            layers.append(output)\n",
    "\n",
    "        return layers[-1]\n",
    "\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        out_channels = int(targets.get_shape()[-1])\n",
    "        outputs = create_generator(inputs, out_channels)\n",
    "\n",
    "    # create two copies of discriminator, one for real pairs and one for fake pairs\n",
    "    # they share the same underlying variables\n",
    "    with tf.name_scope(\"real_discriminator\"):\n",
    "        with tf.variable_scope(\"discriminator\"):\n",
    "            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\n",
    "            predict_real = create_discriminator(inputs, targets)\n",
    "\n",
    "    with tf.name_scope(\"fake_discriminator\"):\n",
    "        with tf.variable_scope(\"discriminator\", reuse=True):\n",
    "            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\n",
    "            predict_fake = create_discriminator(inputs, outputs)\n",
    "\n",
    "    discrim_loss = DISCRIM_LOSS(predict_real, predict_fake)\n",
    "    gen_loss_GAN = GEN_LOSS(predict_fake)\n",
    "    with tf.name_scope(\"generator_loss\"):\n",
    "        if (a.l1_weight == 0.0):\n",
    "            # don't bother to compute what we plan to ignore\n",
    "            gen_loss_L1 = tf.constant(0.0)\n",
    "        else:\n",
    "            gen_loss_L1 = tf.reduce_mean(tf.abs(targets - outputs))\n",
    "        gen_loss = gen_loss_GAN * a.gan_weight + gen_loss_L1 * a.l1_weight\n",
    "\n",
    "    with tf.name_scope(\"discriminator_train\"):\n",
    "        discrim_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"discriminator\")]\n",
    "        discrim_optim = tf.train.AdamOptimizer(a.lr, a.beta1)\n",
    "        discrim_grads_and_vars = discrim_optim.compute_gradients(discrim_loss, var_list=discrim_tvars)\n",
    "        discrim_train = discrim_optim.apply_gradients(discrim_grads_and_vars)\n",
    "\n",
    "    with tf.name_scope(\"generator_train\"):\n",
    "        with tf.control_dependencies([discrim_train]):\n",
    "            gen_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"generator\")]\n",
    "            gen_optim = tf.train.AdamOptimizer(a.lr, a.beta1)\n",
    "            gen_grads_and_vars = gen_optim.compute_gradients(gen_loss, var_list=gen_tvars)\n",
    "            gen_train = gen_optim.apply_gradients(gen_grads_and_vars)\n",
    "\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "    update_losses = ema.apply([discrim_loss, gen_loss_GAN, gen_loss_L1])\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    incr_global_step = tf.assign(global_step, global_step+1)\n",
    "\n",
    "    return Model(\n",
    "        predict_real=predict_real,\n",
    "        predict_fake=predict_fake,\n",
    "        discrim_loss=ema.average(discrim_loss),\n",
    "        discrim_grads_and_vars=discrim_grads_and_vars,\n",
    "        gen_loss_GAN=ema.average(gen_loss_GAN),\n",
    "        gen_loss_L1=ema.average(gen_loss_L1),\n",
    "        gen_grads_and_vars=gen_grads_and_vars,\n",
    "        outputs=outputs,\n",
    "        train=tf.group(update_losses, incr_global_step, gen_train),\n",
    "    )\n",
    "\n",
    "\n",
    "def save_images(fetches, step=None):\n",
    "    image_dir = os.path.join(a.output_dir, \"images\")\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "\n",
    "    filesets = []\n",
    "    for i, in_path in enumerate(fetches[\"paths\"]):\n",
    "        name, _ = os.path.splitext(os.path.basename(in_path.decode(\"utf8\")))\n",
    "        fileset = {\"name\": name, \"step\": step}\n",
    "        description = { \"inputs\": \"edge\", \"targets\": \"base\", \"outputs\": \"faked\" }\n",
    "        for kind in [\"inputs\", \"outputs\", \"targets\"]:\n",
    "            filename = name + \"-\" + description[kind] + \".png\"\n",
    "            if step is not None:\n",
    "                filename = \"%08d-%s\" % (step, filename)\n",
    "            fileset[kind] = filename\n",
    "            out_path = os.path.join(image_dir, filename)\n",
    "            contents = fetches[kind][i]\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                f.write(contents)\n",
    "        filesets.append(fileset)\n",
    "    return filesets\n",
    "\n",
    "\n",
    "def append_index(filesets, step=False):\n",
    "    index_path = os.path.join(a.output_dir, \"index.html\")\n",
    "    if os.path.exists(index_path):\n",
    "        index = open(index_path, \"a\")\n",
    "    else:\n",
    "        index = open(index_path, \"w\")\n",
    "        index.write(\"<html><body><table><tr>\")\n",
    "        if step:\n",
    "            index.write(\"<th>step</th>\")\n",
    "        index.write(\"<th>name</th><th>input</th><th>output</th><th>target</th></tr>\")\n",
    "\n",
    "    for fileset in filesets:\n",
    "        index.write(\"<tr>\")\n",
    "\n",
    "        if step:\n",
    "            index.write(\"<td>%d</td>\" % fileset[\"step\"])\n",
    "        index.write(\"<td>%s</td>\" % fileset[\"name\"])\n",
    "\n",
    "        for kind in [\"inputs\", \"outputs\", \"targets\"]:\n",
    "            index.write(\"<td><img src='images/%s'></td>\" % fileset[kind])\n",
    "\n",
    "        index.write(\"</tr>\")\n",
    "    return index_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    if a.seed is None:\n",
    "        a.seed = random.randint(0, 2**31 - 1)\n",
    "\n",
    "    tf.set_random_seed(a.seed)\n",
    "    np.random.seed(a.seed)\n",
    "    random.seed(a.seed)\n",
    "\n",
    "    if not os.path.exists(a.output_dir):\n",
    "        os.makedirs(a.output_dir)\n",
    "\n",
    "    for k, v in a._get_kwargs():\n",
    "        print(k, \"=\", v)\n",
    "\n",
    "    with open(os.path.join(a.output_dir, \"options.json\"), \"w\") as f:\n",
    "        f.write(json.dumps(vars(a), sort_keys=True, indent=4))\n",
    "\n",
    "    examples = load_examples()\n",
    "    print(\"examples count = %d\" % examples.count)\n",
    "\n",
    "    # inputs and targets are [batch_size, height, width, channels]\n",
    "    model = create_model(examples.inputs, examples.targets)\n",
    "\n",
    "    inputs = examples.inputs\n",
    "    targets = examples.targets\n",
    "    outputs = model.outputs\n",
    "\n",
    "    def convert(image):\n",
    "        return tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True)\n",
    "\n",
    "    # reverse any processing on images so they can be written to disk or displayed to user\n",
    "    with tf.name_scope(\"convert_inputs\"):\n",
    "        converted_inputs = convert(inputs)\n",
    "\n",
    "    with tf.name_scope(\"convert_targets\"):\n",
    "        converted_targets = convert(targets)\n",
    "\n",
    "    with tf.name_scope(\"convert_outputs\"):\n",
    "        converted_outputs = convert(outputs)\n",
    "\n",
    "    with tf.name_scope(\"encode_images\"):\n",
    "        display_fetches = {\n",
    "            \"paths\": examples.paths,\n",
    "            \"inputs\": tf.map_fn(tf.image.encode_png, converted_inputs, dtype=tf.string, name=\"input_pngs\"),\n",
    "            \"targets\": tf.map_fn(tf.image.encode_png, converted_targets, dtype=tf.string, name=\"target_pngs\"),\n",
    "            \"outputs\": tf.map_fn(tf.image.encode_png, converted_outputs, dtype=tf.string, name=\"output_pngs\"),\n",
    "        }\n",
    "\n",
    "    # summaries\n",
    "    with tf.name_scope(\"inputs_summary\"):\n",
    "        tf.summary.image(\"inputs\", converted_inputs)\n",
    "\n",
    "    with tf.name_scope(\"targets_summary\"):\n",
    "        tf.summary.image(\"targets\", converted_targets)\n",
    "\n",
    "    with tf.name_scope(\"outputs_summary\"):\n",
    "        tf.summary.image(\"outputs\", converted_outputs)\n",
    "\n",
    "    with tf.name_scope(\"predict_real_summary\"):\n",
    "        tf.summary.image(\"predict_real\", tf.image.convert_image_dtype(model.predict_real, dtype=tf.uint8))\n",
    "\n",
    "    with tf.name_scope(\"predict_fake_summary\"):\n",
    "        tf.summary.image(\"predict_fake\", tf.image.convert_image_dtype(model.predict_fake, dtype=tf.uint8))\n",
    "\n",
    "    tf.summary.scalar(\"discriminator_loss\", model.discrim_loss)\n",
    "    tf.summary.scalar(\"generator_loss_GAN\", model.gen_loss_GAN)\n",
    "    tf.summary.scalar(\"generator_loss_L1\", model.gen_loss_L1)\n",
    "\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name + \"/values\", var)\n",
    "\n",
    "    for grad, var in model.discrim_grads_and_vars + model.gen_grads_and_vars:\n",
    "        tf.summary.histogram(var.op.name + \"/gradients\", grad)\n",
    "\n",
    "    with tf.name_scope(\"parameter_count\"):\n",
    "        parameter_count = tf.reduce_sum([tf.reduce_prod(tf.shape(v)) for v in tf.trainable_variables()])\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "    logdir = a.output_dir if (a.trace_freq > 0 or a.summary_freq > 0) else None\n",
    "    sv = tf.train.Supervisor(logdir=logdir, save_summaries_secs=0, saver=None)\n",
    "    with sv.managed_session() as sess:\n",
    "        print(\"parameter_count =\", sess.run(parameter_count))\n",
    "\n",
    "        max_steps = 2**32\n",
    "        if a.max_epochs is not None:\n",
    "            max_steps = examples.steps_per_epoch * a.max_epochs\n",
    "        if a.max_steps is not None:\n",
    "            max_steps = a.max_steps\n",
    "\n",
    "        # training\n",
    "        start = time.time()\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            def should(freq):\n",
    "                return freq > 0 and ((step + 1) % freq == 0 or step == max_steps - 1)\n",
    "\n",
    "            options = None\n",
    "            run_metadata = None\n",
    "            if should(a.trace_freq):\n",
    "                options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "\n",
    "            fetches = {\n",
    "                \"train\": model.train,\n",
    "                \"global_step\": sv.global_step,\n",
    "            }\n",
    "\n",
    "            if should(a.progress_freq):\n",
    "                fetches[\"discrim_loss\"] = model.discrim_loss\n",
    "                fetches[\"gen_loss_GAN\"] = model.gen_loss_GAN\n",
    "                fetches[\"gen_loss_L1\"] = model.gen_loss_L1\n",
    "\n",
    "            if should(a.summary_freq):\n",
    "                fetches[\"summary\"] = sv.summary_op\n",
    "\n",
    "            if should(a.display_freq):\n",
    "                fetches[\"display\"] = display_fetches\n",
    "\n",
    "            results = sess.run(fetches, options=options, run_metadata=run_metadata)\n",
    "\n",
    "            if should(a.summary_freq):\n",
    "                print(\"recording summary\")\n",
    "                sv.summary_writer.add_summary(results[\"summary\"], results[\"global_step\"])\n",
    "\n",
    "            if should(a.display_freq):\n",
    "                print(\"saving display images\")\n",
    "                filesets = save_images(results[\"display\"], step=results[\"global_step\"])\n",
    "                append_index(filesets, step=True)\n",
    "\n",
    "            if should(a.trace_freq):\n",
    "                print(\"recording trace\")\n",
    "                sv.summary_writer.add_run_metadata(run_metadata, \"step_%d\" % results[\"global_step\"])\n",
    "\n",
    "            if should(a.progress_freq):\n",
    "                train_epoch = math.ceil(results[\"global_step\"] / examples.steps_per_epoch)\n",
    "                train_step = (results[\"global_step\"] - 1) % examples.steps_per_epoch + 1\n",
    "                rate = (step + 1) * a.batch_size / (time.time() - start)\n",
    "                remaining = (max_steps - step) * a.batch_size / rate\n",
    "                print(\"progress  epoch %d  step %d  image/sec %0.1f  remaining %dm\" % (train_epoch, train_step, rate, remaining / 60))\n",
    "                print(\"discrim_loss\", results[\"discrim_loss\"])\n",
    "                print(\"gen_loss_GAN\", results[\"gen_loss_GAN\"])\n",
    "                print(\"gen_loss_L1\", results[\"gen_loss_L1\"])\n",
    "\n",
    "            if should(a.save_freq):\n",
    "                print(\"saving model\")\n",
    "                saver.save(sess, os.path.join(a.output_dir, \"model\"), global_step=sv.global_step)\n",
    "\n",
    "            if sv.should_stop():\n",
    "                break\n",
    "\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DZyZIFGW-uIE"
   ],
   "name": "pix2pix-in-colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
