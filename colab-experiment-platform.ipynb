{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZeVBSh7N2e9"
   },
   "source": [
    "This notebook satisfies the environmental requirements of running cGAN experiments in Colab while monitoring via TensorBoard.\n",
    "\n",
    "We start off by making Google Drive files visible to this notebook.\n",
    "We then attach TensorBoard and have it monitor an output directory (LOG_DIR).\n",
    "\n",
    "Finally we incorporate an experimental section from which tests can be run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gN-ODgFbOy3T"
   },
   "source": [
    "# Make Google Drive files accessible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rB0TIg-XQryV"
   },
   "source": [
    "See https://colab.research.google.com/notebooks/io.ipynb#scrollTo=jRQ5_yMcqJiV for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-EaX7RdQ04F"
   },
   "source": [
    "This will prompt you to open a new tab that allows Colab access to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31996,
     "status": "ok",
     "timestamp": 1543370722130,
     "user": {
      "displayName": "Jeff Soules",
      "photoUrl": "",
      "userId": "13521288027814780687"
     },
     "user_tz": 300
    },
    "id": "xfM8677QQk-6",
    "outputId": "4afc9ae5-411b-4764-f86e-859c3e4728cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1488,
     "status": "ok",
     "timestamp": 1543370732155,
     "user": {
      "displayName": "Jeff Soules",
      "photoUrl": "",
      "userId": "13521288027814780687"
     },
     "user_tz": 300
    },
    "id": "PUh6WT2QZ-Sq",
    "outputId": "7c760cf0-a9b9-4fdd-cd2b-ef63ebe31ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'My Drive'  'Team Drives'\n"
     ]
    }
   ],
   "source": [
    "!ls '/content/gdrive/'\n",
    "\n",
    "## UNFORTUNATELY, we cannot mount the filesystem\n",
    "## which ought to be rooted in \"Shared with me\"--i.e. shared files.\n",
    "##\n",
    "## The user will need to copy whatever datasets and imports are needed\n",
    "## and set up directory paths directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3348,
     "status": "ok",
     "timestamp": 1543365327674,
     "user": {
      "displayName": "Kelly Marshall",
      "photoUrl": "",
      "userId": "09151661360907445863"
     },
     "user_tz": 300
    },
    "id": "rM6Gge96RVB2",
    "outputId": "e6138042-7257-416d-85f8-7fe414b66711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Google Drive! I am Colab!"
     ]
    }
   ],
   "source": [
    "# OPTIONAL:\n",
    "# Prove that the mountpoint works. Only needs to be run once.\n",
    "# Also it will overwrite anything in your google drive home directory that\n",
    "# was named foo.txt, so maybe don't run it if that's like a wallet with sixty\n",
    "# bitcoins in it or something\n",
    "with open('/content/gdrive/My Drive/foo.txt', 'w') as f:\n",
    "  f.write('Hello Google Drive! I am Colab!')\n",
    "!cat /content/gdrive/My\\ Drive/foo.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Global Parameters\n",
    "This includes command-line arguments and imports.\n",
    "\n",
    "Note that because a Jupyter notebook, particularly in the cloud environment,\n",
    "does not have a command line per se, flag values are passed as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# SET THESE VALUES TO THE PATHS THAT CORRECTLY DESCRIBE YOUR ENVIRONMENT!\n",
    "DRIVE_BASE_DIR = '/content/gdrive/My Drive/2018-Fa Drori Intro ML/Group Project/'\n",
    "HANDBAG_DIR = DRIVE_BASE_DIR + 'handbags/500_bags'\n",
    "FACADE_DIR = DRIVE_BASE_DIR + 'facades/train'\n",
    "## ASSUME THAT \"SLIM\" DIRECTORY AND \"NETWORKS.PY\" FROM GITHUB\n",
    "## ARE STORED SOMEWHERE UNDER LIBDIR\n",
    "LIBDIR = DRIVE_BASE_DIR + 'imports'\n",
    "\n",
    "# Update our includes-path to import other files stored in Drive\n",
    "# Obviously LIBDIR must be defined first!\n",
    "if not os.path.isdir(LIBDIR): raise Exception(\"LIBDIR not set.\")\n",
    "if not LIBDIR in sys.path:\n",
    "  sys.path.append(LIBDIR)\n",
    "if not os.path.isdir(HANDBAG_DIR): raise Exception(\"Default data path not set.\")\n",
    "if not os.path.isdir(FACADE_DIR): raise Exception(\"Default data path not set.\")\n",
    "    \n",
    "import networks\n",
    "from slim.nets import cyclegan\n",
    "from slim.nets import pix2pix\n",
    "\n",
    "\n",
    "\n",
    "# Main TFGAN library.\n",
    "tfgan = tf.contrib.gan\n",
    "CROP_SIZE = 128\n",
    "DEFAULT_SCALE = 140\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#added default argument\n",
    "parser.add_argument(\"--input_dir\",default=\"\", help=\"path to folder containing images\")\n",
    "parser.add_argument(\"--output_dir\",default=\"\", help=\"where to put output files\")\n",
    "\n",
    "parser.add_argument(\"--max_steps\", default=1000,type=int,\n",
    "                    help=\"max number of training steps (0 to disable)\")\n",
    "parser.add_argument(\"--max_epochs\", type=int, help=\"max number of training epochs\")\n",
    "parser.add_argument(\"--summary_freq\", type=int, default=1,\n",
    "                    help=\"update summaries every summary_freq steps\")\n",
    "parser.add_argument(\"--progress_freq\", type=int, default=1,\n",
    "                    help=\"display progress every progress_freq steps\")\n",
    "parser.add_argument(\"--trace_freq\", type=int, default=1,\n",
    "                    help=\"trace execution every trace_freq steps\")\n",
    "parser.add_argument(\"--display_freq\", type=int, default=1,\n",
    "                    help=\"write current training images every display_freq steps\")\n",
    "parser.add_argument(\"--save_freq\", type=int, default=5000,\n",
    "                    help=\"save model every save_freq steps, 0 to disable\")\n",
    "\n",
    "### IMPORTANT PARAMETER ARGUMENTS\n",
    "parser.add_argument(\"--scale_size\", type=int, default=DEFAULT_SCALE,\n",
    "                    help=\"scale images to this size before cropping to CROPSIZE x CROPSIZE\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=2,\n",
    "                    help=\"number of images in batch\")\n",
    "### to be used for extension to pix2pix-style loss function--presently unused\n",
    "parser.add_argument(\"--l1_weight\", type=float, default=100.0, help=\"weight on L1 term for generator gradient\")\n",
    "parser.add_argument(\"--gan_weight\", type=float, default=1.0, help=\"weight on GAN term for generator gradient\")\n",
    "\n",
    "parser.add_argument(\"--n_critic\", type=int, default=5, help=\"Number of training iterations for the discriminator for each generator iteration\")\n",
    "\n",
    "parser.add_argument(\"--loss_fn\", choices=['wgan', 'mod', 'minimax', 'pix2pix'],\n",
    "                    default='wgan', help=\"loss function to use for generator and discriminator.\")\n",
    "parser.add_argument(\"--disc_alpha\", type=float, default=0.0001, help=\"Learning rate for discriminator network\")\n",
    "parser.add_argument(\"--gen_alpha\", type=float, default=0.001, help=\"Learning rate for generator network\")\n",
    "parser.add_argument(\"--dataset\", choices=['handbags', 'facades'],\n",
    "                    default='handbags', help=\"Choose dataset to use\")\n",
    "\n",
    "a = parser.parse_args(args=[\"--n_critic=3\", \"--max_steps=1000\",\n",
    "                            \"--loss_fn=wgan\",\n",
    "                            #\"--disc_alpha=0.0001\",\n",
    "                            \"--dataset=handbags\",\n",
    "                            \"--batch_size=1\" ])\n",
    "if a.output_dir == \"\":\n",
    "  a.output_dir = \"{}-{}-ncrit{}-batch{}-alpha{}-{}\".format(a.dataset, a.max_steps, a.n_critic,\n",
    "                                                           a.batch_size, a.disc_alpha, a.loss_fn)\n",
    "  \n",
    "if a.dataset == 'handbags':\n",
    "  a.input_dir = HANDBAG_DIR\n",
    "elif a.dataset == 'facades':\n",
    "  a.input_dir = FACADE_DIR\n",
    "else:\n",
    "  raise ValueError(\"Unknown dataset.\")\n",
    "\n",
    "print(\"Input: {} Output: {}\".format(a.input_dir, a.output_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PySjYFhMO3PF"
   },
   "source": [
    "# Connect TensorBoard to Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86N4ar4s5qjS"
   },
   "outputs": [],
   "source": [
    "### Here's how you'd do it with ngrok, just here for reference.\n",
    "### We used localtunnel.\n",
    "#!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "#!unzip ngrok-stable-linux-amd64.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tvinklq57aL"
   },
   "outputs": [],
   "source": [
    "#import time\n",
    "#get_ipython().system_raw('./ngrok http 6006 &')\n",
    "#time.sleep(2)\n",
    "\n",
    "#! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "#    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21527,
     "status": "ok",
     "timestamp": 1543365363700,
     "user": {
      "displayName": "Kelly Marshall",
      "photoUrl": "",
      "userId": "09151661360907445863"
     },
     "user_tz": 300
    },
    "id": "NkDVmF-l5XZ-",
    "outputId": "c88fc40f-0b3b-4c36-ef1c-b3b6cd66f04e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25h/tools/node/bin/npm -> /tools/node/lib/node_modules/npm/bin/npm-cli.js\n",
      "\u001b[K\u001b[?25h/tools/node/bin/npx -> /tools/node/lib/node_modules/npm/bin/npx-cli.js\n",
      "\u001b[K\u001b[?25h+ npm@6.4.1\n",
      "added 273 packages from 152 contributors, removed 419 packages and updated 40 packages in 13.862s\n",
      "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/client\n",
      "\u001b[K\u001b[?25h+ localtunnel@1.9.1\n",
      "added 54 packages from 31 contributors in 2.45s\n"
     ]
    }
   ],
   "source": [
    "## Install requirements\n",
    "# (Only need to execute this cell once)\n",
    "! npm i -g npm\n",
    "! npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8533,
     "status": "ok",
     "timestamp": 1543365384517,
     "user": {
      "displayName": "Kelly Marshall",
      "photoUrl": "",
      "userId": "09151661360907445863"
     },
     "user_tz": 300
    },
    "id": "KONzMIB853x4",
    "outputId": "fc939a79-0794-41a9-d68c-ed04725aa743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: tensorboard --logdir /content/gdrive/My Drive/Group Project/log-fullRun/ --host 0.0.0.0 --port 6006 &\n",
      "Connect to TensorBoard from wherever at this URL:\n",
      "your url is: https://hot-grasshopper-20.localtunnel.me\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Now launch TensorBoard pointing to the globally configured log directory.\n",
    "print(\"Executing: tensorboard --logdir {} --host 0.0.0.0 --port 6006 &\"\n",
    "  .format(a.output_dir))\n",
    "# Shell out to execute tensorboard\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir \"{}\" --host 0.0.0.0 --port 6006 &'\n",
    "    .format(a.output_dir)\n",
    ")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Tunnel port 6006 (assume TensorBoard ran successfully)\n",
    "get_ipython().system_raw('lt --port 6006 > url.txt 2>&1 &')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "print (\"Connect to TensorBoard from wherever at this URL:\")\n",
    "! cat url.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qP3RTpREWFHG"
   },
   "source": [
    "If you change the logging directory, you'll need to kill\n",
    "the existing TensorBoard process and start\n",
    "a new one. (So far as I can tell, restarting won't succeed; there can only be one\n",
    "process listening on a port at a time anyway.)\n",
    "\n",
    "The following cell will do that (you need to run it, look at the output, then modify the \"kill\" line and uncomment it so it will actually kill the process.) The rerun of ps is a double-check that you did successfully shut TensorBoard down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9182,
     "status": "ok",
     "timestamp": 1543273705731,
     "user": {
      "displayName": "Kelly Marshall",
      "photoUrl": "",
      "userId": "09151661360907445863"
     },
     "user_tz": 300
    },
    "id": "NLiPtrZ0WXYn",
    "outputId": "34d6ac69-e7c3-4529-af36-4b2802620686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PID TTY          TIME CMD\n",
      "    291 ?        00:02:54 tensorboard\n",
      "    291 ?        00:02:55 tensorboard\n"
     ]
    }
   ],
   "source": [
    "## RUN THIS CELL ONLY IF YOU NEED TO RESTART TENSORBOARD (like to point to a new log dir)\n",
    "import time\n",
    "!ps | head -n 1\n",
    "!ps | grep 'tensorboard'\n",
    "## Read the \"PID\" column to figure out what process number TensorBoard is,\n",
    "## then use that processs in the next line\n",
    "# !kill PID_GOES_HERE\n",
    "time.sleep(2)\n",
    "!ps | grep 'tensorboard'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "-MQSA-gS-JEX"
   },
   "source": [
    "The following cell cleans out the log directory *LOG_DIR*.\n",
    "\n",
    "Uncomment the \"os.unlink\" and \"shutil.rmtree\" lines to actually delete\n",
    "(they're commented as safety so you don't delete work by running the cell accidentally.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1543273708070,
     "user": {
      "displayName": "Kelly Marshall",
      "photoUrl": "",
      "userId": "09151661360907445863"
     },
     "user_tz": 300
    },
    "id": "qU9t94gP7ak5",
    "outputId": "2f0b5bee-f887-450e-9d27-411322f6cb42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlinking /content/gdrive/My Drive/Group Project/log-fullRun/graph.pbtxt\n",
      "Recursively removing /content/gdrive/My Drive/Group Project/log-fullRun/images\n",
      "Unlinking /content/gdrive/My Drive/Group Project/log-fullRun/index.html\n",
      "Unlinking /content/gdrive/My Drive/Group Project/log-fullRun/events.out.tfevents.1543241235.59feb57c6f7e\n",
      "Unlinking /content/gdrive/My Drive/Group Project/log-fullRun/events.out.tfevents.1543271338.943eaeaf319f\n",
      "Unlinking /content/gdrive/My Drive/Group Project/log-fullRun/events.out.tfevents.1543271566.943eaeaf319f\n",
      "Unlinking /content/gdrive/My Drive/Group Project/log-fullRun/events.out.tfevents.1543273566.943eaeaf319f\n"
     ]
    }
   ],
   "source": [
    "# Run this ONLY to clear out old log files.\n",
    "# Commented out the actual deleters to avoid accidental execution.\n",
    "import os, shutil\n",
    "for the_file in os.listdir(LOG_DIR):\n",
    "    file_path = os.path.join(LOG_DIR, the_file)\n",
    "    try:\n",
    "        if os.path.isfile(file_path):\n",
    "            print(\"Unlinking {}\".format(file_path))\n",
    "#            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            print (\"Recursively removing {}\".format(file_path))\n",
    "#            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZyZIFGW-uIE"
   },
   "source": [
    "## Test It\n",
    "\n",
    "Only need to execute this to prove that the tensorboard connection is working.\n",
    "\n",
    "If you do run this, be sure to clean out LOG_DIR afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1006
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36169,
     "status": "ok",
     "timestamp": 1543270571269,
     "user": {
      "displayName": "Kelly Marshall",
      "photoUrl": "",
      "userId": "09151661360907445863"
     },
     "user_tz": 300
    },
    "id": "qyclV58D51l7",
    "outputId": "f6e92ee3-1d02-4b1c-ccd2-6acf29082467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-cf6a558d68e8>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: 0001 cost= 1.184038759\n",
      "Epoch: 0002 cost= 0.665434184\n",
      "Epoch: 0003 cost= 0.552909983\n",
      "Epoch: 0004 cost= 0.498737923\n",
      "Epoch: 0005 cost= 0.465547395\n",
      "Epoch: 0006 cost= 0.442658992\n",
      "Epoch: 0007 cost= 0.425608423\n",
      "Epoch: 0008 cost= 0.412207377\n",
      "Epoch: 0009 cost= 0.401463278\n",
      "Epoch: 0010 cost= 0.392418534\n",
      "Epoch: 0011 cost= 0.384726579\n",
      "Epoch: 0012 cost= 0.378187135\n",
      "Epoch: 0013 cost= 0.372420243\n",
      "Epoch: 0014 cost= 0.367342757\n",
      "Epoch: 0015 cost= 0.362765909\n",
      "Epoch: 0016 cost= 0.358620076\n",
      "Epoch: 0017 cost= 0.354928667\n",
      "Epoch: 0018 cost= 0.351470063\n",
      "Epoch: 0019 cost= 0.348320921\n",
      "Epoch: 0020 cost= 0.345376467\n",
      "Epoch: 0021 cost= 0.342740766\n",
      "Epoch: 0022 cost= 0.340266723\n",
      "Epoch: 0023 cost= 0.337926292\n",
      "Epoch: 0024 cost= 0.335771112\n",
      "Epoch: 0025 cost= 0.333646116\n",
      "Training done!\n",
      "Accuracy: 0.9134\n",
      "Now try connecting to the address from the previous cell.\n"
     ]
    }
   ],
   "source": [
    "#### EXAMPLE JUST TO PROVE IT'S WORKING\n",
    "### DON'T RUN THIS UNLESS YOU JUST WANT TO TEST TENSORBOARD-IN-THE-CLOUD\n",
    "### IF YOU DO, RUN THE \"CLEAN OUT LOG DIR\" CELL AFTERWARD\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./tmp/data/\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_epoch = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "with tf.name_scope('Model'):\n",
    "    pred = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "with tf.name_scope('Loss'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "tf.summary.scalar(\"accuracy\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(LOG_DIR, graph=tf.get_default_graph())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # optimize/backprop, do cost (compute loss), summarize\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            avg_cost += c / total_batch\n",
    "        if (epoch+1) % display_epoch == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\",\n",
    "                  \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print (\"Training done!\")\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y:mnist.test.labels}))\n",
    "    print(\"Now try connecting to the address from the previous cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgI7H5NS-9ma"
   },
   "source": [
    "# Experiment Code\n",
    "\n",
    "This is the main code for whatever you want to test!\n",
    "If you have used a custom argument parser above, you could use\n",
    "whatever experiment you want to run down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DZyZIFGW-uIE"
   ],
   "name": "pix2pix-in-colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
